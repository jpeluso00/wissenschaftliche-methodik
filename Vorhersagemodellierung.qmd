---
title: "Fahrradvermietungen"
subtitle: "Prognosemodell"
lang: de
author: "Marcel Albers, Koen Loogman, Jacques Peluso und Steffen Seegler"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    html-math-method: katex
  pdf:
    toc: true
    number-sections: false
    colorlinks: true 
    papersize: a4
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
library(mosaic)
library(here)
library(tidyverse)
library(psych)
library(keras3)
library(tensorflow)
library(Metrics)

# Run once for setup
# install_tensorflow()
tf$config$list_physical_devices("GPU")
```

<!-- Sollten Umlaute falsch angezeigt werden: "File -> Reopen with Encoding -> UTF8" -->

# Einleitung

<!-- Umfang von 0,5-1,5 Seiten -->


# Explorative Datenanalyse und Datenvorverarbeitung

<!-- Umfang von 2-4 Seiten -->

Daten einlesen:

```{r}
# Load data
raw.application <- here("data", "raw", "anwendung.csv") |> read.csv2()
raw.dataset <- here("data", "raw", "train.csv") |> read.csv2()

# Show the structure of the data
str(raw.application)
str(raw.dataset)
```

## Einfuehrungsphase

Handelt es sich um die Einführungsphase, d.h., ist das Angebot neu?

```{r}
gf_bar(~ einfuehrungsphase, data=raw.dataset)
```

### Vermietungen in Abhängigkeit zur Einfuehrungsphase

```{r}
gf_boxplot(vermietungen ~ einfuehrungsphase, data=raw.dataset)
```

## Jahreszeit

Jahreszeit. Kann man nicht viel mehr dazu sagen.

```{r}
gf_bar(~ jahreszeit, data=raw.dataset)
```

### Vermietungen in Abhängigkeit zur Jahreszeit

```{r}
gf_boxplot(vermietungen ~ jahreszeit, data=raw.dataset)
```

## Wetter

Die Wetterbeschreibung.

```{r}
gf_bar(~ wetter, data=raw.dataset)
```

### Vermietungen in Abhängigkeit zum Wetter

```{r}
gf_boxplot(vermietungen ~ wetter, data=raw.dataset)
```

## Arbeitstag

Handelt es sich um einen Werktag oder um Wochenende bzw. Feier- oder Ferientag?

```{r}
gf_bar(~ arbeitstag, data=raw.dataset)
```

### Vermietungen in Abhängigkeit zum Arbeitstag

```{r}
gf_boxplot(vermietungen ~ arbeitstag, data=raw.dataset)
```

## Temperatur

Temperatur in °C.

```{r}
fav_stats(raw.dataset$temperatur)
gf_dhistogram(~ temperatur, data=raw.dataset) |>
  gf_dens(linewidth = 2)
```

### Vermietungen in Abhängigkeit zur Temperatur

```{r}
gf_point(vermietungen ~ temperatur, data=raw.dataset) |>
  gf_smooth()
```

## Windgeschwindigkeit

Windgeschwindigkeit in km/h.

```{r}
fav_stats(raw.dataset$windgeschwindigkeit)
gf_dhistogram(~ windgeschwindigkeit, data=raw.dataset) |>
  gf_dens(linewidth = 2)
```

### Vermietungen in Abhängigkeit zur Temperatur

```{r}
gf_point(vermietungen ~ windgeschwindigkeit, data=raw.dataset) |>
  gf_smooth()
```

## Luftfeuchtigkeit

Luftfeuchtigkeit in %.

```{r}
fav_stats(raw.dataset$luftfeuchtigkeit)
gf_dhistogram(~ luftfeuchtigkeit, data=raw.dataset) |>
  gf_dens(linewidth = 2)
```

### Vermietungen in Abhängigkeit zur Luftfeuchtigkeit

```{r}
gf_point(vermietungen ~ luftfeuchtigkeit, data=raw.dataset) |>
  gf_smooth()
```

## Vermietungen

Anzahl Vermitungen.

```{r}
fav_stats(raw.dataset$vermietungen)
gf_dhistogram(~ vermietungen, data=raw.dataset) |>
  gf_dens(linewidth = 2)
```


## Preprocessing der Daten

Quelle: https://www.geeksforgeeks.org/data-preprocessing-in-r/

```{r}
preprocess <- function(dataset) {
  # Make a copy of the data
  dataset.copy <- dataset

  # Encode einfuehrungsphase and arbeitstag (binary encoding)
  yn_mapping <- c("Nein" = 0, "Ja" = 1)
  dataset.copy$einfuehrungsphase <- yn_mapping[as.character(dataset$einfuehrungsphase)]
  dataset.copy$arbeitstag <- yn_mapping[as.character(dataset$arbeitstag)]

  # Encode wetter (ordinal encoding)
  wetter_mapping <- c("Schlecht" = 0, "Nicht gut" = 1, "Gut" = 2)
  dataset.copy$wetter <- wetter_mapping[as.character(dataset$wetter)]

  # Encode jahreszeit (one-hot encoding)
  encoded_jahreszeit <- as.data.frame(model.matrix(~ jahreszeit - 1, dataset))

  # Add the new columns to the encoded data frame and remove the old column
  dataset.copy <- dataset.copy |>
    select(-jahreszeit)
  dataset.copy <- cbind(encoded_jahreszeit, dataset.copy)

  return(dataset.copy)
}
```

### Preprocessing Ergebnisse

```{r}
# Make this example reproducible
set.seed(69)

# Preprocess the data
application <- preprocess(raw.application)
dataset <- preprocess(raw.dataset)

# Data before preprocessing
head(raw.dataset)

# Data after preprocessing
head(dataset)

# Prepare the data for training
dataset_features <- dataset |> select(-vermietungen)
dataset_labels <- dataset |> select(vermietungen)

# Use 80% of dataset for training and 20% for testing
dataset.split <- sample(c(TRUE, FALSE), nrow(dataset), replace=TRUE, prob=c(0.8, 0.2))

train.raw.dataset = raw.dataset[dataset.split, ]
val.raw.dataset = raw.dataset[!dataset.split, ]

train.dataset = dataset[dataset.split, ]
val.dataset = dataset[!dataset.split, ]

train.dataset_features = dataset_features[dataset.split, ]
train.dataset_labels = dataset_labels[dataset.split, ]
val.dataset_features = dataset_features[!dataset.split, ]
val.dataset_labels = dataset_labels[!dataset.split, ]
```

# Methodenbeschreibung

<!-- Umfang von 2-4 Seiten -->

https://www.datacamp.com/tutorial/neural-network-models-r

Gehen Sie hier auf die verwendete Methode zur Modellierung, Variablen, und Modellauswahl ein. Zitieren Sie hier auch die methodische Literatur.

# Anwendung, Ergebnis und Vorhersage

<!-- Umfang von 2-4 Seiten -->

Wenden Sie hier Ihr Modell an und Interpretieren Sie Ihr Ergebnis. Bei Einzelarbeiten sollte der reine Text (ohne Code, Abbildungen etc.) einen Umfang von ca. 1--2 Seiten haben, bei Gruppenarbeiten einen von ca. 2--4 Seiten.

## Modell mit den Daten trainieren

### Base-Line Modell
```{r}
# Train a simple lm on all the data
lm.model <- lm(vermietungen ~ ., data = train.raw.dataset)
summary(lm.model)

# Calculate the MAE to compare models
mae(val.raw.dataset$vermietungen, predict(lm.model, newdata = val.raw.dataset))
```

### Unser Modell

```{r}
# Set a random seed for training
set_random_seed(69, disable_gpu = FALSE)

checkpoint_path <- "training_nn/cp.ckpt.weights.h5"
checkpoint_dir <- fs::path_dir(checkpoint_path)

# Adapt a normalizer on the data as we didn't normalize it yet
normalizer <- layer_normalization(axis = -1L)
normalizer |> adapt(as.matrix(dataset_features))

# Define the input layer
input <- layer_input(shape = dim(dataset_features)[2])

# Define the feed forward and output
output <- input |>
  normalizer() |>
  layer_dropout(rate = 0.05) |>
  layer_dense(units = 32, activation = 'relu') |>
  layer_dense(units = 32, activation = 'relu') |>
  layer_dense(units = 1)

# Build the model
nn.model <- keras_model(input, output)

# Assign the optimizer and the metrics to use (mse and mae for regression)
nn.model |> compile(
  optimizer = 'adam',
  loss = 'mse',
  metrics = list('mae')
)

# Create a callback that saves the model's weights
cp_callback <- callback_model_checkpoint(
  filepath = checkpoint_path,
  save_weights_only = TRUE,
  save_best_only = TRUE,
)

# Train the model with a 20% validation split
nn.model |>
  fit(
    as.matrix(train.dataset_features),
    as.matrix(train.dataset_labels),
    epochs = 5000,
    batch_size = 64,
    verbose = FALSE,
    validation_data = list(as.matrix(val.dataset_features), as.matrix(val.dataset_labels)),
    callbacks = list(cp_callback),
  ) |>
  plot()

# Loads the weights
load_model_weights(nn.model, checkpoint_path)

# Evaluate model
nn.model |>
  evaluate(
    as.matrix(val.dataset_features),
    as.matrix(val.dataset_labels),
    batch_size = 64
  )
```

## Modell zur Vorhersage anwenden

```{r}
# Predict with the model
predictions <- nn.model(as.matrix(application))

# Add predictions to the application
pred.application <- raw.application
pred.application$vermietungen <-as.integer(predictions)
```

Kontrolle und Export:

```{r}
# Save results
write.csv2(pred.application, file = "Prognose_IhrName.csv")
```


# 6. Zusammenfassung

<!-- Umfang von 0,5-1 Seiten -->

Fassen Sie kurz die zentralen Ergebnisse zusammen (0,5--1 Seite). Gehen Sie auch auf die Grenzen Ihrer Analyse ein.

*Hinweis:* Laden Sie das aus der qmd-Datei gerenderte pdf-Dokument als Prüfungsleistung im OC hoch (keine gedruckte Abgabe erforderlich). Als Zusatzmaterial laden Sie die qmd-Datei und die csv-Datei mit den Daten Ihrer Prognose (`Prognose_IhrName.csv`) hoch.


# Literatur

Hier stehen die im Text verwendeten Quellen:

- Nachname Autor1, Anfangsbuchstabe Vorname Autor1, Nachname Autor2, Anfangsbuchstabe Vorname Autor2 1 & Nachname Autor3, Anfangsbuchstabe Vorname Autor3 (Jahr der Veröffentlichung). Titel des Beitrags. Weitere Publikationsinformationen.

Im Text werden die Quellen nach der Harvard-Intext-Zitation in Klammern angegeben (siehe Leitfaden). 

**ChatGPT & Co:** Beachten Sie die Hinweise dazu im Leitfaden zur formalen Gestaltung von Seminar- und Abschlussarbeiten, Stand 24/01.
